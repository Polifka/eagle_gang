{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all relevant libraries\n",
    "\n",
    "* install haversine ($ pip install haversine ; https://pypi.org/project/haversine/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"GnBu_d\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define readin functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_data_readin (city):\n",
    "    trip_data = pd.read_csv(\"Project_Data/Trip_Data/\"+city+\".csv\", encoding = \"ISO-8859-1\")\n",
    "    trip_data = trip_data_data_prep(trip_data)\n",
    "    trip_data = merge_trip_data_with_weather_data(trip_data, city)\n",
    "    return trip_data\n",
    "\n",
    "\n",
    "def trip_data_data_prep(trip_data):\n",
    "    trip_data[\"datetime_start\"] = pd.to_datetime(trip_data['day'] + ' ' + trip_data['time'])\n",
    "    trip_data[\"trip_duration\"] = pd.to_timedelta(trip_data[\"trip_duration\"])\n",
    "    trip_data[\"datetime_end\"] = trip_data[\"datetime_start\"] + trip_data[\"trip_duration\"]\n",
    "    trip_data[\"weekday\"] = pd.to_datetime(trip_data[\"datetime_start\"]).dt.weekday\n",
    "    trip_data[\"month\"] = trip_data[\"datetime_start\"].dt.month\n",
    "    trip_data[\"hour\" ] = trip_data[\"datetime_start\"].dt.hour\n",
    "    trip_data[\"week\" ] = trip_data[\"datetime_start\"].dt.isocalendar().week\n",
    "    \n",
    "    trip_data = estimate_fleetsize(trip_data,\"week\")\n",
    "    trip_data = estimate_fleetsize(trip_data,\"day\")\n",
    "    trip_data = estimate_fleetsize(trip_data,\"month\")\n",
    "    \n",
    "    trip_data = distance_between_coordinates (trip_data)    \n",
    "    trip_data = cleaning(trip_data)\n",
    "    return trip_data\n",
    "\n",
    "def distance_between_coordinates (trip_data):\n",
    "    trip_data[\"distance\"] = trip_data.apply(lambda row: haversine((row[\"orig_lat\"], row[\"orig_lng\"]),(row[\"dest_lat\"], row[\"dest_lng\"]),Unit.KILOMETERS), axis=1) \n",
    "    return trip_data\n",
    "\n",
    "def estimate_fleetsize(trip_data, time_resolution):\n",
    "    temp = pd.DataFrame(trip_data.groupby(time_resolution)[\"b_number\"].nunique())\n",
    "    temp = temp.rename(columns={\"b_number\": \"fleetsize_\"+time_resolution+\"ly\"})\n",
    "    trip_data =pd.merge(trip_data, temp , on=time_resolution, how='left')\n",
    "    return trip_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_trip_data_with_weather_data (trip_data, city):\n",
    "    trip_data.set_index(pd.DatetimeIndex(trip_data[\"datetime_start\"]),inplace=True)\n",
    "    weather = read_DWD_data (city)\n",
    "    trip_data[\"rounded_time_hourly\"] = trip_data[\"datetime_start\"].dt.round(\"H\")\n",
    "    combined = pd.merge(trip_data, weather , on=\"rounded_time_hourly\", how='left')\n",
    "    combined[\"wind\"] = combined['wind'].fillna(method=\"ffill\")\n",
    "    combined[\"rain\"] = combined['rain'].fillna(method=\"ffill\")\n",
    "    combined[\"temp\"] = combined['temp'].fillna(method=\"ffill\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def read_DWD_data (city):\n",
    "    \n",
    "    temp_data = pd.read_csv(\"Project_Data/Weather_Data/\"+city+\"/\"+\"data_TT_TU_MN009.csv\", encoding = \"ISO-8859-1\")\n",
    "    temp_data =temp_data.rename(columns={\"Zeitstempel\": \"rounded_time_hourly\"})\n",
    "    temp_data.set_index((pd.to_datetime(temp_data['rounded_time_hourly'].astype(str), format='%Y%m%d%H%M') ),inplace=True)\n",
    "    temp_data=temp_data.drop(['Produkt_Code', 'SDO_ID','SDO_ID','Qualitaet_Niveau','Qualitaet_Byte','rounded_time_hourly'], axis=1)\n",
    "    temp_data.columns = ['temp']\n",
    "\n",
    "\n",
    "    wind_data = pd.read_csv(\"Project_Data/Weather_Data/\"+city+\"/\"+\"data_F_MN003.csv\", encoding = \"ISO-8859-1\")\n",
    "    wind_data =wind_data.rename(columns={\"Zeitstempel\": \"rounded_time_hourly\"})\n",
    "    wind_data.set_index((pd.to_datetime(wind_data['rounded_time_hourly'].astype(str), format='%Y%m%d%H%M') ),inplace=True)\n",
    "    wind_data = wind_data.drop(['Produkt_Code', 'SDO_ID','SDO_ID','Qualitaet_Niveau','Qualitaet_Byte','rounded_time_hourly'], axis=1)\n",
    "    wind_data.columns = ['wind']\n",
    "\n",
    "    \n",
    "    rain_data = pd.read_csv(\"Project_Data/Weather_Data/\"+city+\"/\"+\"data_R1_MN008.csv\", encoding = \"ISO-8859-1\")\n",
    "    rain_data =rain_data.rename(columns={\"Zeitstempel\": \"rounded_time_hourly\"})\n",
    "    rain_data.set_index((pd.to_datetime(rain_data['rounded_time_hourly'].astype(str), format='%Y%m%d%H%M') ),inplace=True)\n",
    "    rain_data = rain_data.drop(['Produkt_Code', 'SDO_ID','SDO_ID','Qualitaet_Niveau','Qualitaet_Byte','rounded_time_hourly'], axis=1)\n",
    "    rain_data.columns = ['rain']\n",
    "    \n",
    "    \n",
    "    weather = pd.merge(temp_data, wind_data , on=\"rounded_time_hourly\", how='left')\n",
    "    weather = pd.merge(weather, rain_data , on=\"rounded_time_hourly\", how='left')\n",
    "    return weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting trips with avg. speed over 25km/h\n",
    "def cleaning (trip_data):\n",
    "    trip_data[\"trip_duration_hours\"] = trip_data[\"trip_duration\"].dt.total_seconds()/3600\n",
    "    trip_data[\"avg_speed\"] = trip_data[\"distance\"]/trip_data[\"trip_duration_hours\"]\n",
    "    trip_data.drop(trip_data[trip_data[\"avg_speed\"]>25].index, axis=0, inplace=True)\n",
    "    return trip_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create combined Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export (city_name):\n",
    "    data_set = trip_data_readin(city_name)\n",
    "    data_set.to_csv(\"Project_Data/Combined_Data/\"+city_name+\".csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatetimeProperties' object has no attribute 'isocalendar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a9295ca77963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# execute only in case of new data updates #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"duesseldorf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mexport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"bremen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e47555aea650>\u001b[0m in \u001b[0;36mexport\u001b[1;34m(city_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_data_readin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Project_Data/Combined_Data/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-db5d6e496c61>\u001b[0m in \u001b[0;36mtrip_data_readin\u001b[1;34m(city)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrip_data_readin\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Project_Data/Trip_Data/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_data_data_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrip_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_trip_data_with_weather_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrip_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-db5d6e496c61>\u001b[0m in \u001b[0;36mtrip_data_data_prep\u001b[1;34m(trip_data)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datetime_start\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hour\"\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datetime_start\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"week\"\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datetime_start\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misocalendar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtrip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_fleetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrip_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"week\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatetimeProperties' object has no attribute 'isocalendar'"
     ]
    }
   ],
   "source": [
    "# execute only in case of new data updates #\n",
    "\n",
    "export (\"duesseldorf\")\n",
    "export (\"bremen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = trip_data_readin(\"bremen\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fleet Size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section deals with the calculation of the fleet size of the respective cities, which will be used as a basis for the calculation of other key performance indices (KPI). Since the fleet size is not covered by the data, we approximate the fleet size based on the number of bikes in a given period. To assess this period, the following fleet sizes were first calculated:\n",
    "\n",
    "1. **Fleet size per week:** Assuming that a bike that does not appear in the list of trips for a week is no longer part of the fleet.\n",
    "2. **Fleet size per month:** Assuming that the fleet only includes bikes that have been driven within one month.\n",
    "3. **Fleet size per day:** Based on the assumption that bicycles that have not been used for a day are no longer part of the fleet.\n",
    "\n",
    "### Read data\n",
    "\n",
    "Reading the data per city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(city_name):\n",
    "    trip_data = pd.read_csv(\"Project_Data/Combined_Data/\"+city_name+\".csv\", encoding = \"ISO-8859-1\")\n",
    "    trip_data[\"trip_duration\"] = pd.to_timedelta(trip_data[\"trip_duration\"])\n",
    "    trip_data[\"datetime_start\"] = pd.to_datetime(trip_data[\"datetime_start\"])\n",
    "    trip_data[\"datetime_end\"] = pd.to_datetime(trip_data[\"datetime_end\"])\n",
    "    trip_data[\"day\"] = pd.to_datetime(trip_data[\"day\"])\n",
    "\n",
    "\n",
    "    return trip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate fleet sizes\n",
    "\n",
    "Calculate and plot fleet sizes based on time resolution and fleet data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_fleet_size (fleet, timeresolution):\n",
    "    fleet[\"day\"] = pd.to_datetime(fleet[\"day\"],format='%Y%m%d')\n",
    "    fleet[\"week\"] = fleet[\"day\"].dt.isocalendar().week\n",
    "\n",
    "\n",
    "    first = fleet[\"day\"].iloc[0]\n",
    "    last = fleet[\"day\"].iloc[-1]\n",
    "    i = (last-first)/np.timedelta64(1,'D')\n",
    "    test = first + pd.to_timedelta(np.arange(int(i)+1), 'D')\n",
    "    plot = pd.DataFrame(test)\n",
    "    plot = plot.set_index(test)\n",
    "    plot = pd.DataFrame((fleet.groupby(timeresolution)[\"b_number\"].nunique()))\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_fleet_size(fleets, cities, timeresolution): \n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(15,5), dpi= 80)\n",
    "    \n",
    "    daily_fleet_size = get_daily_fleet_size(fleet[0],timeresolution)\n",
    "    ax.plot(daily_fleet_size.index, daily_fleet_size[\"b_number\"], color=\"green\")\n",
    "    mean_fleet_size = round(daily_fleet_size[\"b_number\"].mean(),2)\n",
    "    print(\"Average fleet size for \" + cities[0] + \" per \" + timeresolution + \": \" + str(mean_fleet_size))\n",
    "    \n",
    "    daily_fleet_size = get_daily_fleet_size(fleet[1],timeresolution)\n",
    "    ax.plot(daily_fleet_size.index, daily_fleet_size[\"b_number\"], color=\"blue\")\n",
    "    mean_fleet_size = round(daily_fleet_size[\"b_number\"].mean(),2)\n",
    "    print(\"Average fleet size for \" + cities[1] + \" per \" + timeresolution + \": \" + str(mean_fleet_size))\n",
    "    \n",
    "    ax.set_xlabel(timeresolution+\"s\")\n",
    "    ax.set_ylabel(\"Number of bikes per \"+ timeresolution)\n",
    "    ax.set_title(\"Number of bikes per \"+timeresolution )\n",
    "    ax.legend([\"Duesseldorf\", \"Bremen\"]);\n",
    "\n",
    "    plt.savefig(\"Project_Data/Graphics\"+timeresolution+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize calculated fleet sizes per city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fleet_dus = read_data(\"duesseldorf\")\n",
    "fleet_bre = read_data(\"bremen\")\n",
    "\n",
    "cities = ['Duesseldorf','Bremen']\n",
    "fleets = [fleet_dus, fleet_bre]\n",
    "\n",
    "plot_daily_fleet_size(fleets, cities, \"day\")\n",
    "plot_daily_fleet_size(fleets, cities, \"week\")\n",
    "plot_daily_fleet_size(fleets, cities, \"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous visualizations, all following calculations are based on the assumption that a bike that has not been used for more than one week is no longer part of the fleet. A daily calculation of the fleet size seems to be too sensitive to reflect the reality due to the high fluctuations in the key figure. A monthly calculation seems too rough to be used as a basis for calculating other key figures.\n",
    "Possible reasons for the removal of bikes from the fleet could be theft or longer repairs of the bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
